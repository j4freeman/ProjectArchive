{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.205846Z",
     "start_time": "2019-09-02T14:40:38.698691Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from IPython.display import Image, Audio, clear_output\n",
    "\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "import os\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import time\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "from sklearn import preprocessing, decomposition, cluster, manifold, neighbors, metrics\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imageio\n",
    "\n",
    "import shutil\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import community as cm\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import seaborn as sbs\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.398002Z",
     "start_time": "2019-09-02T14:40:47.207561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "GPU = True\n",
    "device_idx = 1\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx - 1) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.401928Z",
     "start_time": "2019-09-02T14:40:47.398936Z"
    }
   },
   "outputs": [],
   "source": [
    "# network = nx.Graph(nx.read_graphml(\"dyngem_graphs/0.graphml\"))\n",
    "\n",
    "# attrs = [nx.get_node_attributes(network, 'apt_markakis')[i] for i in network.nodes]\n",
    "\n",
    "# nx.draw_kamada_kawai(network, node_size=15, node_color=attrs)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.469781Z",
     "start_time": "2019-09-02T14:40:47.402925Z"
    }
   },
   "outputs": [],
   "source": [
    "def wider_input(new_dim, ae):\n",
    "    new_layers = new_dim - ae.linear1.lin.weight.shape[1]\n",
    "\n",
    "    if new_layers <= 0:\n",
    "#         print(\"returning\")\n",
    "        return ae\n",
    "    new_indices = list(np.random.choice(ae.linear1.lin.weight.shape[1], new_layers, replace=True))\n",
    "\n",
    "    weights = ae.linear1.lin.weight\n",
    "    weights = torch.transpose(weights, 0, 1)\n",
    "    biases = ae.linear1.lin.bias\n",
    "\n",
    "    counts = [(i, new_indices.count(i)) for i in new_indices]\n",
    "\n",
    "\n",
    "    new_weights = []\n",
    "\n",
    "    for i in counts:\n",
    "        weights[i[0]] *= 1/(1+i[1])\n",
    "        new_weight = (weights[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_weights.append(new_weight)\n",
    "\n",
    "    new_weights = torch.cat(new_weights)\n",
    "    new_weights = torch.cat([weights, new_weights])\n",
    "#     new_weights = new_weights + torch.randn(new_weights.shape).to(device) * 0.1\n",
    "    new_weights = torch.transpose(new_weights, 0, 1)\n",
    "\n",
    "    biases = biases * ae.linear1.lin.weight.shape[1] / new_dim\n",
    "#     biases = biases + torch.randn(biases.shape).to(device) * 0.1\n",
    "\n",
    "#     ae.linear1.lin.in_features = new_dim\n",
    "    ae.linear1.in_channels = new_dim\n",
    "    ae.linear1.lin.in_channels = new_dim\n",
    "    ae.linear1.lin.bias = torch.nn.Parameter(biases)\n",
    "    ae.linear1.lin.weight = torch.nn.Parameter(new_weights)\n",
    "    return ae\n",
    "    \n",
    "def wider_output(new_dim, ae):\n",
    "    \n",
    "    new_layers = new_dim - ae.linear4.lin.weight.shape[0]\n",
    "    if new_layers <= 0:\n",
    "        return ae\n",
    "    new_indices = list(np.random.choice(ae.linear4.lin.weight.shape[0], new_layers, replace=True))\n",
    "\n",
    "    weights = ae.linear4.lin.weight\n",
    "#     weights = torch.transpose(weights, 0, 1)\n",
    "    biases = ae.linear4.lin.bias\n",
    "\n",
    "    counts = [(i, new_indices.count(i)) for i in new_indices]\n",
    "\n",
    "    new_biases = []\n",
    "    new_weights = []\n",
    "\n",
    "    for i in counts:\n",
    "        weights[i[0]] *= 1/(1+i[1])\n",
    "        new_weight = (weights[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_weights.append(new_weight + torch.randn(new_weight.shape).to(device) * 0.1)\n",
    "        \n",
    "        biases[i[0]] *= 1/(1+i[1])\n",
    "        new_bias = (biases[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_biases.append(new_bias + torch.randn(new_bias.shape).to(device) * 0.1)\n",
    "\n",
    "\n",
    "    new_weights = torch.cat(new_weights)\n",
    "    new_weights = torch.cat([weights, new_weights])\n",
    "#     new_weights = torch.transpose(new_weights, 0, 1)\n",
    "#     new_weights = new_weights + torch.randn(new_weights.shape).to(device) * 0.1\n",
    "    \n",
    "    new_biases = torch.cat(new_biases).view(-1)\n",
    "    new_biases = torch.cat([biases, new_biases])\n",
    "#     new_biases = new_biases + torch.randn(new_biases.shape).to(device) * 0.1\n",
    "\n",
    "    ae.linear4.out_features = new_dim\n",
    "    ae.linear4.out_channels = new_dim\n",
    "    ae.linear4.lin.bias = torch.nn.Parameter(new_biases)\n",
    "    ae.linear4.lin.weight = torch.nn.Parameter(new_weights)\n",
    "    return ae\n",
    "\n",
    "def wider_decode(new_dim, ae):\n",
    "        \n",
    "    new_layers = new_dim - ae.decode_linear.weight.shape[0]\n",
    "    if new_layers <= 0:\n",
    "        return ae\n",
    "    new_indices = list(np.random.choice(ae.decode_linear.weight.shape[0], new_layers, replace=True))\n",
    "\n",
    "    weights = ae.decode_linear.weight\n",
    "#     weights = torch.transpose(weights, 0, 1)\n",
    "    biases = ae.decode_linear.bias\n",
    "\n",
    "    counts = [(i, new_indices.count(i)) for i in new_indices]\n",
    "\n",
    "    new_biases = []\n",
    "    new_weights = []\n",
    "\n",
    "    for i in counts:\n",
    "        weights[i[0]] *= 1/(1+i[1])\n",
    "        new_weight = (weights[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_weights.append(new_weight + torch.randn(new_weight.shape).to(device) * 0.01)\n",
    "        \n",
    "        biases[i[0]] *= 1/(1+i[1])\n",
    "        new_bias = (biases[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_biases.append(new_bias + torch.randn(new_bias.shape).to(device) * 0.01)\n",
    "\n",
    "\n",
    "    new_weights = torch.cat(new_weights)\n",
    "    new_weights = torch.cat([weights, new_weights])\n",
    "#     new_weights = torch.transpose(new_weights, 0, 1)\n",
    "#     new_weights = new_weights + torch.randn(new_weights.shape).to(device) * 0.1\n",
    "    \n",
    "    new_biases = torch.cat(new_biases).view(-1)\n",
    "    new_biases = torch.cat([biases, new_biases])\n",
    "#     new_biases = new_biases + torch.randn(new_biases.shape).to(device) * 0.1\n",
    "\n",
    "    ae.decode_linear.out_features = new_dim\n",
    "    ae.decode_linear.out_channels = new_dim\n",
    "    ae.decode_linear.bias = torch.nn.Parameter(new_biases)\n",
    "    ae.decode_linear.weight = torch.nn.Parameter(new_weights)\n",
    "    return ae\n",
    "\n",
    "def wider_cheb_input(new_dim, ae):\n",
    "    \n",
    "    print('input layer moving from ', ae.linear1.weight.shape)\n",
    "    print('to ', new_dim)\n",
    "        \n",
    "    new_layers = new_dim - ae.linear1.weight.shape[1]\n",
    "    if new_layers <= 0:\n",
    "        return ae\n",
    "    new_indices = list(np.random.choice(ae.linear1.weight.shape[1], new_layers, replace=True))\n",
    "\n",
    "    weights = ae.linear1.weight.detach()\n",
    "#     weights = torch.transpose(weights, 0, 1)\n",
    "#     biases = ae.linear1.bias\n",
    "\n",
    "    counts = [(i, new_indices.count(i)) for i in new_indices]\n",
    "\n",
    "#     new_biases = []\n",
    "    new_weights = []\n",
    "\n",
    "    for i in counts:\n",
    "        weights[:, i[0]] *= 1/(1+i[1])\n",
    "        new_weight = (weights[:, i[0]] * 1/(1+i[1])).view(weights.shape[0],1,-1)\n",
    "        \n",
    "        new_weights.append(new_weight + torch.randn(new_weight.shape).to(device) * 0.01)\n",
    "        \n",
    "#         biases[i[0]] *= 1/(1+i[1])\n",
    "#         new_bias = (biases[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "#         new_biases.append(new_bias + torch.randn(new_bias.shape).to(device) * 0.1)\n",
    "\n",
    "\n",
    "    new_weights = torch.cat(new_weights, dim=1)\n",
    "    new_weights = torch.cat([weights, new_weights], dim=1)\n",
    "    \n",
    "#     new_weights = torch.transpose(new_weights, 0, 1)\n",
    "#     new_weights = new_weights + torch.randn(new_weights.shape).to(device) * 0.1\n",
    "    \n",
    "#     new_biases = torch.cat(new_biases).view(-1)\n",
    "#     new_biases = torch.cat([biases, new_biases])\n",
    "#     new_biases = new_biases + torch.randn(new_biases.shape).to(device) * 0.1\n",
    "\n",
    "    ae.linear1.in_channels = new_dim\n",
    "#     ae.linear1.bias = torch.nn.Parameter(new_biases)\n",
    "    ae.linear1.weight = torch.nn.Parameter(new_weights)\n",
    "    return ae\n",
    "\n",
    "def wider_cheb_output(new_dim, ae):\n",
    "        \n",
    "    new_layers = new_dim - ae.linear6.weight.shape[2]\n",
    "    if new_layers <= 0:\n",
    "        return ae\n",
    "    new_indices = list(np.random.choice(ae.linear6.weight.shape[2], new_layers, replace=True))\n",
    "\n",
    "    weights = ae.linear6.weight.detach()\n",
    "#     weights = torch.transpose(weights, 0, 1)\n",
    "    biases = ae.linear6.bias.detach()\n",
    "\n",
    "    counts = [(i, new_indices.count(i)) for i in new_indices]\n",
    "\n",
    "    new_biases = []\n",
    "    new_weights = []\n",
    "\n",
    "    for i in counts:\n",
    "        weights[:, :, i[0]] *= 1/(1+i[1])\n",
    "        new_weight = (weights[:, :, i[0]] * 1/(1+i[1])).view(weights.shape[0],-1,1)\n",
    "        new_weights.append(new_weight + torch.randn(new_weight.shape).to(device) * 0.01)\n",
    "        \n",
    "        biases[i[0]] *= 1/(1+i[1])\n",
    "        new_bias = (biases[i[0]] * 1/(1+i[1])).view(1,-1)\n",
    "        new_biases.append(new_bias + torch.randn(new_bias.shape).to(device) * 0.01)\n",
    "\n",
    "\n",
    "    new_weights = torch.cat(new_weights, dim=2)\n",
    "    \n",
    "    new_weights = torch.cat([weights, new_weights], dim=2)\n",
    "    \n",
    "#     new_weights = torch.transpose(new_weights, 0, 1)\n",
    "#     new_weights = new_weights + torch.randn(new_weights.shape).to(device) * 0.1\n",
    "    \n",
    "    new_biases = torch.cat(new_biases).view(-1)\n",
    "    new_biases = torch.cat([biases, new_biases])\n",
    "#     new_biases = new_biases + torch.randn(new_biases.shape).to(device) * 0.1\n",
    "\n",
    "    ae.linear6.out_channels = new_dim\n",
    "    ae.linear6.bias = torch.nn.Parameter(new_biases)\n",
    "    ae.linear6.weight = torch.nn.Parameter(new_weights)\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.488947Z",
     "start_time": "2019-09-02T14:40:47.471741Z"
    }
   },
   "outputs": [],
   "source": [
    "def neg_sin(x):\n",
    "    return 0.025 * (torch.sin(x) - torch.cos(x))\n",
    "\n",
    "def SineReLU(x):\n",
    "    x[x < 0] = neg_sin(x[x < 0])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.506648Z",
     "start_time": "2019-09-02T14:40:47.490691Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, input_dim, mid_layer, corrupt=0.01):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        \n",
    "        self.linear1 = gnn.ChebConv(input_dim, mid_layer, 3)\n",
    "        self.b1 = nn.BatchNorm1d(mid_layer)\n",
    "        self.conv2 = nn.Conv1d(1, hidden_size, 5, stride=2)\n",
    "        self.b2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.conv3 = nn.Conv1d(hidden_size, int(hidden_size/2), 5, stride=2)\n",
    "        self.b3 = nn.BatchNorm1d(int(hidden_size/2))\n",
    "        self.conv4 = nn.Conv1d(int(hidden_size/2), 1, 3)\n",
    "        num_lin = 123\n",
    "        self.b4 = nn.BatchNorm1d(num_lin)\n",
    "        self.linear1a = gnn.ChebConv(num_lin, hidden_size, 2)\n",
    "        \n",
    "        self.linear2 = gnn.ChebConv(hidden_size, num_lin, 2)\n",
    "        self.b5 = nn.BatchNorm1d(num_lin)\n",
    "        self.conv5 = nn.ConvTranspose1d(1, int(hidden_size/2), 3)\n",
    "        self.b6 = nn.BatchNorm1d(int(hidden_size/2))\n",
    "        self.conv6 = nn.ConvTranspose1d(int(hidden_size/2), hidden_size, 5, stride=2, output_padding=1)\n",
    "        self.b7 = nn.BatchNorm1d(hidden_size)\n",
    "        self.conv7 = nn.ConvTranspose1d(hidden_size, 1, 5, stride=2, output_padding=1)\n",
    "        self.b8 = nn.BatchNorm1d(mid_layer)\n",
    "        self.linear6 = gnn.ChebConv(mid_layer, input_dim, 3)\n",
    "                                \n",
    "#         self.activ = nn.ReLU()\n",
    "        self.activ = SineReLU\n",
    "    \n",
    "        self.sp_linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def encode(self, x, edges):\n",
    "        \n",
    "        x = self.linear1(x, edges)\n",
    "        x = self.b1(x)\n",
    "        x = self.activ(x)\n",
    "        \n",
    "        x = x.view(-1, 1, x.shape[1])\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.activ(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.activ(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.b4(x)\n",
    "        x = self.activ(x)\n",
    "        \n",
    "        mu = self.linear1a(x, edges)\n",
    "        \n",
    "        return mu\n",
    "    \n",
    "    \n",
    "    def decode(self, z, edges):\n",
    "        \n",
    "        noise = torch.rand(z.shape)\n",
    "        z[noise < 1 / z.shape[1]] = 0\n",
    "        \n",
    "        z = self.linear2(z, edges)\n",
    "        z = self.activ(z)\n",
    "        z = self.b5(z)\n",
    "        z = z.view(-1, 1, z.shape[1])\n",
    "        z = self.conv5(z)\n",
    "        z = self.activ(z)\n",
    "        z = self.b6(z)\n",
    "        z = self.conv6(z)\n",
    "        z = self.activ(z)\n",
    "        z = self.b7(z)\n",
    "        z = self.conv7(z)\n",
    "        z = z.squeeze()\n",
    "        z = self.linear6(z, edges)\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    def sp(self, data):\n",
    "        return self.sp_linear(data)\n",
    "\n",
    "    def forward(self, d):\n",
    "        x, edges = d.x, d.edge_index\n",
    "                \n",
    "        x = x.float()\n",
    "        mu = self.encode(x, edges)\n",
    "        recon2 = self.decode(mu, edges)\n",
    "        \n",
    "        return recon2, mu, None, None, recon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.523602Z",
     "start_time": "2019-09-02T14:40:47.507645Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_walk(graph, node, num_transitions=5):\n",
    "    cur = node\n",
    "    nodes = []\n",
    "    adj_list = []\n",
    "    for i in range(num_transitions):\n",
    "        nodes.append(cur)\n",
    "        if np.random.rand() < 0.1:\n",
    "            cur = node\n",
    "        neighbors = list(graph.neighbors(cur))\n",
    "\n",
    "        if neighbors:\n",
    "            cur = np.random.choice(neighbors)\n",
    "        else:\n",
    "            cur = node\n",
    "        \n",
    "    return [int(x) for x in nodes]\n",
    "\n",
    "def walk_matrix(graph, num_transitions=5):\n",
    "    adj_tensor = torch.zeros(graph.number_of_nodes(), graph.number_of_nodes())\n",
    "    target_tensor = torch.ones(adj_tensor.shape).to(device).long() * -1\n",
    "#     for i in range(graph.number_of_nodes()):\n",
    "    for idx, i in enumerate(list(graph.nodes)):\n",
    "#         neighbors = graph[str(i)]\n",
    "        neighbors = graph[i]\n",
    "        walk = random_walk(graph, i, num_transitions)\n",
    "        walk += [int(x) for x in neighbors]\n",
    "        walk = list(set(walk) - set([i]))\n",
    "        adj_tensor[idx, walk] = 1\n",
    "        target_tensor[idx,:len(walk)] = torch.tensor(walk)\n",
    "                \n",
    "    adj_tensor = adj_tensor.to(device).double()\n",
    "    return adj_tensor, target_tensor\n",
    "\n",
    "# random_walk(network, '0')\n",
    "# len(random_walk(network, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.541556Z",
     "start_time": "2019-09-02T14:40:47.524599Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_pheromones(graph):\n",
    "    for u, v in graph.edges:\n",
    "        if not 'pheromone' in graph[u][v]:\n",
    "            graph[u][v]['pheromone'] = 1\n",
    "\n",
    "def increase_pheromone(graph, cur, new):\n",
    "    cur_lev = graph[cur][new]['pheromone']\n",
    "    if cur_lev + 0.1 > 5:\n",
    "        graph[cur][new]['pheromone'] = 5\n",
    "    else:\n",
    "        graph[cur][new]['pheromone'] += 0.1\n",
    "        \n",
    "def decay_weights(graph, decay):\n",
    "    for u, v in graph.edges:\n",
    "        cur_lev = graph[u][v]['pheromone']\n",
    "        if cur_lev - decay < 1:\n",
    "            graph[u][v]['pheromone'] = 1\n",
    "        else:\n",
    "            graph[u][v]['pheromone'] -= decay\n",
    "    \n",
    "def random_ant(graph, node, num_transitions=5):\n",
    "    cur = node\n",
    "    nodes = []\n",
    "    adj_list = []\n",
    "    for i in range(num_transitions):\n",
    "        nodes.append(cur)\n",
    "        neighbors = list(graph.neighbors(cur))\n",
    "        weights = [graph.get_edge_data(cur, n)['pheromone'] for n in neighbors]\n",
    "        weights = [x / sum(weights) for x in weights]\n",
    "        new = np.random.choice(neighbors, p=weights)\n",
    "        increase_pheromone(graph, cur, new)\n",
    "        cur = new\n",
    "    decay_weights(graph, num_transitions * 0.1 / graph.number_of_nodes())\n",
    "        \n",
    "    return [int(x) for x in nodes]\n",
    "\n",
    "def ant_matrix(graph, num_transitions=5):\n",
    "    return walk_matrix(graph, num_transitions)\n",
    "    adj_tensor = torch.zeros(graph.number_of_nodes(), graph.number_of_nodes())\n",
    "    target_tensor = torch.ones(adj_tensor.shape).to(device).long() * -1\n",
    "    for i in range(graph.number_of_nodes()):\n",
    "        neighbors = graph[str(i)]\n",
    "        walk = random_ant(graph, str(i), num_transitions)\n",
    "        walk += [int(x) for x in neighbors]\n",
    "        walk = list(set(walk) - set([i]))\n",
    "        adj_tensor[i, walk] = 1\n",
    "        target_tensor[i,:len(walk)] = torch.tensor(walk)\n",
    "                \n",
    "    adj_tensor = adj_tensor.to(device).double()\n",
    "    return adj_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.559508Z",
     "start_time": "2019-09-02T14:40:47.543556Z"
    }
   },
   "outputs": [],
   "source": [
    "def modularity_loss(graph, preds):\n",
    "    B = torch.tensor(nx.modularity_matrix(graph)).to(device).float()\n",
    "    Q = torch.transpose(preds, 0, 1) @ B @ preds\n",
    "    loss = torch.trace(Q) / graph.number_of_edges()\n",
    "    return 1000 * torch.norm((torch.transpose(preds, 0, 1) @ preds) - torch.tensor(np.identity(preds.shape[1])).to(device).float(), p='fro') - loss\n",
    "\n",
    "def save_fig(graph, embeddings, layout, init):\n",
    "    \n",
    "    tsne = manifold.TSNE(n_components=2, random_state=0, init=init)\n",
    "    \n",
    "    embeddings = embeddings.detach().to(\"cpu\")\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "    attrs = [nx.get_node_attributes(graph, 'apt_markakis')[i] for i in graph.nodes]\n",
    "#     attrs = [0 for _ in graph.nodes]\n",
    "\n",
    "    embeddings_2 = tsne.fit_transform(embeddings)\n",
    "\n",
    "    ax1.scatter(embeddings_2[:,0], embeddings_2[:,1], c=attrs)\n",
    "#     ax1.scatter(embeddings_2[:,0], embeddings_2[:,1])\n",
    "\n",
    "    plt.setp((ax1, ax2), xticks=np.linspace(-0.1, 0.5, 0.05), xticklabels=np.linspace(-0.1, 0.5, 0.05),\n",
    "            yticks=np.linspace(-0.1, 0.5, 0.05), yticklabels=np.linspace(-0.1, 0.5, 0.05))\n",
    "\n",
    "    nx.draw(graph, node_color=attrs, pos=layout, node_size=20, ax=ax2)\n",
    "#     nx.draw(graph, pos=layout, node_size=20, ax=ax2)\n",
    "    \n",
    "    plt.savefig('runs/embeddings/' + str(int(time.time())) + '.png')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close('all')\n",
    "    \n",
    "    return embeddings_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:47.577461Z",
     "start_time": "2019-09-02T14:40:47.560513Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neighbors(i):\n",
    "    neighbors = network.neighbors(i)\n",
    "    try:\n",
    "        return int(np.random.choice([n for n in neighbors]))\n",
    "    except:\n",
    "        return int(i)\n",
    "    \n",
    "def safe_shortest_path(G, u, v):\n",
    "    try:\n",
    "        return nx.shortest_path_length(G, u, v)\n",
    "    except:\n",
    "        return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:40:55.300941Z",
     "start_time": "2019-09-02T14:40:47.579457Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('runs/embeddings'):\n",
    "    shutil.rmtree('runs/embeddings')\n",
    "os.makedirs('runs/embeddings/')\n",
    "\n",
    "init_epochs = 50\n",
    "learning_rate = 0.005\n",
    "length = 1\n",
    "start = 1400\n",
    "nets = [nx.read_graphml('smallworld/time_graphs/0/' + str(start + i * 10) + '.graphml') for i in range(length)]\n",
    "# nets = [nx.read_graphml('hepth/cora.graphml')]\n",
    "network = nets[0]\n",
    "model = VariationalEncoder(hidden_size = 16, \n",
    "                      input_dim = nets[0].number_of_nodes(), \n",
    "                      mid_layer=512).to(device)\n",
    "\n",
    "#     network = nx.read_graphml(\"runs/graphs/\" + str(start + 10 * (m)) + \".graphml\")\n",
    "#     network = nx.read_graphml('smallworld/time_graphs/0/'+ str(start + 10*m) + '.graphml')\n",
    "#         network = nx.read_graphml(\"dyngem_graphs/\" + str(m) + \".graphml\")\n",
    "#         network = nx.read_graphml(\"hepth/graphs/\" + str(m) + \".graphml\")\n",
    "#         network = nx.read_graphml('hepth/blogcatalog/graph.graphml')\n",
    "#         network = nx.read_graphml('smallworld/small_world.graphml')\n",
    "#         network = nx.read_edgelist('smallworld/real_graphs/facebook_combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afe26a87ced4c558ed8d6cd4ec46faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:  0.011139840262510758 walk len:  12\n",
      "transitioning: 1022 -> 1022\n",
      "input layer moving from  torch.Size([3, 1022, 512])\n",
      "to  1022\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "init_t = time.time()\n",
    "times = []\n",
    "\n",
    "edge_losses = []\n",
    "label_losses = []\n",
    "total_losses = []\n",
    "\n",
    "all_embeds = []\n",
    "\n",
    "init = 'random'\n",
    "\n",
    "fig_name = 0\n",
    "for m in tqdm_notebook(range(length)):\n",
    "    network = nets[m]\n",
    "    \n",
    "    adj_list = [(int(n), [int(x) for x in list(neighbors)]) for n, neighbors in network.adjacency()]\n",
    "    \n",
    "    adj_list = [(x[0], y) for x in adj_list for y in x[1]]\n",
    "    \n",
    "    adj_list = torch.tensor(adj_list).to(device)\n",
    "    adj_list = torch.transpose(adj_list, 0, 1)\n",
    "    \n",
    "    degrees = dict(network.degree()).values()\n",
    "#     sparsity = sum(degrees) / (len(degrees)**2)\n",
    "    sparsity = 2 * network.number_of_edges() / (network.number_of_nodes() * (network.number_of_nodes() - 1))\n",
    "    print(\"sparsity: \", sparsity, \"walk len: \", math.ceil(sparsity * network.number_of_nodes()))\n",
    "    model.corrupt = sparsity \n",
    "    \n",
    "    attr_tensor, targets = ant_matrix(network, num_transitions=math.ceil(sparsity * network.number_of_nodes()))\n",
    "    \n",
    "#     attr_tensor, targets = walk_matrix(network, num_transitions=int(sparsity * network.number_of_nodes()))\n",
    "    \n",
    "    print(\"transitioning:\", model.linear1.weight.shape[1], \"->\", attr_tensor.shape[0])\n",
    "\n",
    "    data = Data(x=attr_tensor, edge_index=adj_list).to(device)\n",
    "#     model = wider_input(attr_tensor.shape[0], model)\n",
    "    model = wider_cheb_input(attr_tensor.shape[0], model)\n",
    "#     model = wider_output(attr_tensor.shape[0], model)\n",
    "    model = wider_cheb_output(attr_tensor.shape[0], model)\n",
    "    \n",
    "#     optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    neg_list = [list(set(network.nodes) - set([n for n in network.neighbors(i)])) for i in network.nodes]\n",
    "    \n",
    "#     prods = torch.tensor([nx.get_node_attributes(network, 'apt_markakis')[i] for i in network.nodes]).to(device).view(-1, 1)\n",
    "    \n",
    "    layout = nx.kamada_kawai_layout(network)\n",
    "\n",
    "    targets = torch.ones(attr_tensor.shape).to(device).long() * -1\n",
    "\n",
    "    for i in range(network.number_of_nodes()):\n",
    "        neighbors = network[str(i)]\n",
    "        walk = list(set(neighbors) - set([i]))\n",
    "        walk = [int(x) for x in walk]\n",
    "        targets[i,:len(walk)] = torch.tensor(walk)\n",
    "    \n",
    "    if m == 0:\n",
    "        epochs = init_epochs * 1\n",
    "    else:\n",
    "        epochs = init_epochs\n",
    "        \n",
    "    for i in tqdm_notebook(range(epochs)):\n",
    "        \n",
    "        attr_tensor, _ = ant_matrix(network, num_transitions=math.ceil(sparsity * network.number_of_nodes()))\n",
    "        data = Data(x=attr_tensor.requires_grad_(False), edge_index=adj_list.requires_grad_(False)).to(device)\n",
    "                        \n",
    "        optim.zero_grad()\n",
    "        recons, embeddings, var, z, _ = model(data)\n",
    "        recons = recons.double()\n",
    "        \n",
    "#         l1 = 0.\n",
    "        \n",
    "#         neighbors = [int(np.random.choice([n for n in network.neighbors(i)])) for i in network.nodes]\n",
    "        neighbors = [get_neighbors(i) for i in network.nodes]\n",
    "        \n",
    "#         for p in model.parameters():\n",
    "#             if p.requires_grad:\n",
    "#                 l1 += torch.sum(torch.abs(p))\n",
    "                \n",
    "#         l1 *= 0.0\n",
    "#         l1 /= sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "#         l1 = l1.to(device).float()\n",
    "        \n",
    "        recon_loss = nn.MultiLabelMarginLoss()(recons, targets).float()\n",
    "#         recon_loss = nn.BCEWithLogitsLoss()(recons, attr_tensor).float()\n",
    "\n",
    "        neighbor_loss = 0\n",
    "        if neighbors:\n",
    "            neighbor_loss = nn.CosineEmbeddingLoss()(embeddings, embeddings[neighbors], torch.ones(embeddings.shape[0]).to(device))\n",
    "#             neighbor_loss /= 2\n",
    "\n",
    "        for _ in range(25):\n",
    "#             negatives = [int(np.random.choice(i)) for i in neg_list]\n",
    "            negatives = np.random.choice(len(neg_list), len(neg_list))\n",
    "            neighbor_loss += nn.CosineEmbeddingLoss()(embeddings, embeddings[negatives], torch.ones(embeddings.shape[0]).to(device) * -1) / 25\n",
    "        \n",
    "#         attr_loss = nn.MSELoss()(preds, prods) * 0.25\n",
    "\n",
    "    \n",
    "        loss = recon_loss + neighbor_loss\n",
    "#         loss = recon_loss + l1 + neighbor_loss\n",
    "        \n",
    "        label_losses.append(recon_loss.item())\n",
    "        edge_losses.append(neighbor_loss.item())\n",
    "        total_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "#         if (i+1)%10 == 0:\n",
    "        if True:\n",
    "            \n",
    "            vals = nx.get_edge_attributes(network, 'pheromone').values()\n",
    "            \n",
    "            print(\"Total Loss: {:.5f} | Recon Loss: {:.5f} | Edge Loss: {:.5f}\".format(\n",
    "                loss.item(), recon_loss.item(), neighbor_loss.item()))\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if type(init) != str and init.shape[0] != network.number_of_nodes():\n",
    "            avg = np.average(init, axis=0).reshape(1, init.shape[1])\n",
    "#             print(network.number_of_nodes() - init.shape[0])\n",
    "            for _ in range(network.number_of_nodes() - init.shape[0]):\n",
    "                init = np.append(init, avg + np.random.normal(size=avg.shape), axis=0)\n",
    "        \n",
    "        init = save_fig(network, embeddings, layout, init)\n",
    "    \n",
    "    times.append(time.time() - init_t)\n",
    "    all_embeds.append(embeddings.detach().to(\"cpu\").numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.730Z"
    }
   },
   "outputs": [],
   "source": [
    "# pickle.dump((times, all_embeds, total_losses), open('walkruns/' + str(init_epochs) + '.pkl', 'wb'))\n",
    "plt.plot(list(range(len(label_losses))), label_losses)\n",
    "plt.title(\"Recon\")\n",
    "plt.show()\n",
    "    \n",
    "plt.plot(list(range(len(edge_losses))), edge_losses)\n",
    "plt.title(\"Neighbor\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(len(total_losses))), total_losses)\n",
    "plt.title(\"Total\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.731Z"
    }
   },
   "outputs": [],
   "source": [
    "layout = nx.layout.kamada_kawai_layout(network)\n",
    "# layout = nx.spring_layout(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.733Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# embeddings = model.encode(data.x.float(), data.edge_index).detach().to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    adj_mat = torch.tensor(nx.to_numpy_matrix(network)).to(device)\n",
    "    _, embeddings, _, z, decoded = model(Data(adj_mat.float(), data.edge_index))\n",
    "    # decoded, embeddings, var, z = model(data)\n",
    "model.train()\n",
    "\n",
    "decoded = decoded.detach().to(\"cpu\")\n",
    "embeddings = embeddings.detach().to(\"cpu\")\n",
    "\n",
    "# rands = np.random.choice(decoded.shape[0], 6)\n",
    "# print(decoded[rands])\n",
    "# print(adj_mat[rands])\n",
    "# print(embeddings[rands])\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "# attrs = [nx.get_node_attributes(network, 'classes')[i] for i in network.nodes]\n",
    "attrs = [nx.get_node_attributes(network, 'apt_markakis')[i] for i in network.nodes]\n",
    "# attrs = [0 for _ in network.nodes]\n",
    "\n",
    "embeddings_2 = manifold.TSNE(n_components=2).fit_transform(embeddings)\n",
    "# embeddings_2 = decomposition.PCA(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "ax1.scatter(embeddings_2[:,0], embeddings_2[:,1], c=attrs, cmap='Dark2')\n",
    "# ax1.scatter(embeddings_2[:,0], embeddings_2[:,1])\n",
    "\n",
    "plt.setp((ax1, ax2), xticks=np.linspace(-0.1, 0.5, 0.05), xticklabels=np.linspace(-0.1, 0.5, 0.05),\n",
    "        yticks=np.linspace(-0.1, 0.5, 0.05), yticklabels=np.linspace(-0.1, 0.5, 0.05))\n",
    "\n",
    "\n",
    "# plt.sca(ax1)\n",
    "# plt.xticks(np.linspace(-0.1, 0.5, 0.05))\n",
    "# plt.yticks(np.linspace(-0.1, 0.5, 0.05))\n",
    "\n",
    "nx.draw(network, node_color=attrs, pos=layout, node_size=20, ax=ax2)\n",
    "# nx.draw(network, pos=layout, node_size=20, ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.735Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# attrs = [nx.get_node_attributes(network, 'apt_markakis')[i] for i in network.nodes]\n",
    "# print(\"true attrs\")\n",
    "# nx.draw(network, node_color=attrs, pos=layout, node_size=20)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"reconstructed attrs\")\n",
    "# nx.draw(network, node_color=preds.squeeze(), pos=layout, node_size=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.737Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "network = nx.Graph(network)\n",
    "communities = cm.best_partition(network)\n",
    "best_vec = [communities.get(node) for node in network.nodes()]\n",
    "mod = cm.modularity(communities,network)\n",
    "community_vec = np.zeros(network.number_of_nodes())\n",
    "# for idx, c in enumerate(communities):\n",
    "#     c = [int(x) for x in c]\n",
    "#     community_vec[c] = idx\n",
    "    \n",
    "nx.draw(network, pos=layout, node_size=10, node_color=best_vec, cmap='hsv')\n",
    "# plt.savefig('communities.png')\n",
    "num_coms = len(set(communities.values()))\n",
    "plt.show()\n",
    "print(\"best communities\")\n",
    "print(mod)\n",
    "\n",
    "print(\"num clusters\", num_coms)\n",
    "\n",
    "mods = []\n",
    "model.eval()\n",
    "for _ in tqdm_notebook(range(5)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, embeddings, var, z, decoded = model(data)\n",
    "        embeddings = embeddings.detach().to(\"cpu\")\n",
    "    \n",
    "    clusters = cluster.KMeans(10).fit_predict(embeddings)\n",
    "    cluster_dict = dict([(list(network.nodes)[i], clusters[i]) for i in range(len(clusters))])\n",
    "    mods.append(cm.modularity(cluster_dict,network))\n",
    "        \n",
    "    print(mods[-1])\n",
    "    \n",
    "    clusters = cluster.KMeans(int(num_coms)).fit_predict(embeddings)\n",
    "    cluster_dict = dict([(list(network.nodes)[i], clusters[i]) for i in range(len(clusters))])\n",
    "    mods.append(cm.modularity(cluster_dict,network))\n",
    "        \n",
    "    print(mods[-1])\n",
    "    \n",
    "    clusters = cluster.KMeans(int(num_coms/2)).fit_predict(embeddings)\n",
    "    cluster_dict = dict([(list(network.nodes)[i], clusters[i]) for i in range(len(clusters))])\n",
    "    mods.append(cm.modularity(cluster_dict,network))\n",
    "        \n",
    "    print(mods[-1])\n",
    "    \n",
    "    print('============')\n",
    "    \n",
    "# clusters = cluster.AgglomerativeClustering(num_coms, connectivity=nx.to_numpy_matrix(network)).fit_predict(embeddings)\n",
    "# mods.append(cm.modularity(cluster_dict,network))\n",
    "nx.draw(network, pos=layout, node_size=10, node_color=clusters, cmap='hsv')\n",
    "plt.show()\n",
    "print(\"k means communities\")\n",
    "print(sum(mods) / len(mods), max(mods))\n",
    "# # plt.savefig('kmeans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.739Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(embeddings_2[:,0], embeddings_2[:,1], c=clusters, cmap='tab20c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.740Z"
    }
   },
   "outputs": [],
   "source": [
    "files = sorted([int(x[:-4]) for x in os.listdir('runs/embeddings/')])\n",
    "files = [str(x) + '.png' for x in files]\n",
    "\n",
    "with imageio.get_writer('runs/embedding_gifs/' + str(int(time.time())) + '.gif', mode='I', duration=0.5) as writer:\n",
    "    for f in files:\n",
    "        filename = 'runs/embeddings/' + f\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.742Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "ks = [50, 250, 500, 1000, network.number_of_edges()]\n",
    "\n",
    "with torch.no_grad():\n",
    "    adj_mat = torch.tensor(nx.to_numpy_matrix(network)).to(device)\n",
    "    _, embeddings, _, z, decoded = model(Data(adj_mat.float(), data.edge_index))\n",
    "        \n",
    "#     decoded = torch.zeros(decoded.shape).to(device)\n",
    "        \n",
    "#     for i in range(embeddings.shape[0]):\n",
    "#         vals = nn.MSELoss(reduction='none')(embeddings[i].repeat(embeddings.shape[0], 1), embeddings)\n",
    "#         decoded[i] = torch.sum(vals, dim=1)\n",
    "    \n",
    "#     decoded += torch.eye(decoded.shape[0]).to(device).float() * 1000\n",
    "    \n",
    "#     decoded[decoded < 0.5] = 1000\n",
    "    \n",
    "# torch.topk(decoded.view(-1), k=10, largest=False)\n",
    "\n",
    "true_edges = list(network.edges())\n",
    "\n",
    "for k in ks:\n",
    "\n",
    "    topks = torch.topk(decoded.view(-1), k=k, largest=True)\n",
    "    topks = topks.values.tolist()\n",
    "    \n",
    "    indices = (decoded >= topks[-1]).nonzero().tolist()\n",
    "    \n",
    "    print(len(indices))\n",
    "    \n",
    "    indices = [(str(x[0]), str(x[1])) for x in indices]\n",
    "\n",
    "#     print(indices[:5])\n",
    "#     print(list(network.edges())[:5])\n",
    "\n",
    "    tp = list(set(indices).intersection(set(true_edges)))\n",
    "\n",
    "    print(\"prec \" + str(k) + \":\", len(tp) / len(indices))\n",
    "    print(\"===========\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.744Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = (decoded > 0).nonzero().tolist()\n",
    "indices = [(str(x[0]), str(x[1])) for x in indices]\n",
    "\n",
    "true_edges = list(network.edges())\n",
    "\n",
    "tp = list(set(indices).intersection(set(true_edges)))\n",
    "    \n",
    "# tp = [x for x in indices if x in list(network.edges())]\n",
    "\n",
    "print(\"prec -1:\", len(tp) / len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.745Z"
    }
   },
   "outputs": [],
   "source": [
    "perfs = []\n",
    "clusts = []\n",
    "for _ in range(10):\n",
    "    clusters = cluster.KMeans(7, n_init=250).fit_predict(embeddings.to(\"cpu\").detach())\n",
    "\n",
    "    knns = [[] for _ in range(7)]\n",
    "    true_vals = [[] for _ in range(7)]\n",
    "    for i in range(len(clusters)):\n",
    "            knns[clusters[i]].append(i)\n",
    "            true_vals[attrs[i]].append(i)\n",
    "\n",
    "    intersects = []\n",
    "\n",
    "    for i in range(7):\n",
    "        intersects.append([len(set(knns[i]).intersection(set(true_vals[j]))) / len(knns[i]) for j in range(7)])\n",
    "\n",
    "    intersects = np.array(intersects)\n",
    "\n",
    "    vals = []\n",
    "    perms = list(itertools.permutations(list(range(7))))\n",
    "    for i in perms:\n",
    "        vals.append(sum(intersects[range(7), i]))\n",
    "\n",
    "    new = perms[vals.index(max(vals))]\n",
    "\n",
    "    remap = dict([(i, new[i]) for i in range(7)])\n",
    "\n",
    "    clusters = [remap[i] for i in clusters]\n",
    "\n",
    "    plt.scatter(embeddings_2[:,0], embeddings_2[:,1], c=clusters, cmap='Dark2')\n",
    "    plt.show()\n",
    "\n",
    "    sbs.heatmap(intersects[new, :])\n",
    "    plt.show()\n",
    "\n",
    "    print(max(vals)/7)\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.747Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(embeddings_2[:,0], embeddings_2[:,1], c=attrs, cmap='Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.750Z"
    }
   },
   "outputs": [],
   "source": [
    "# n2v_embeds = pickle.load(open('hepth/w2v_cora.pkl', 'rb'))[0]\n",
    "n2v_embeds = pickle.load(open('DynamicGEM-master/CORA75.pkl', 'rb'))[1][0]\n",
    "\n",
    "n2v_tsner = manifold.TSNE(n_components=2, n_iter=2500)\n",
    "n2v_tsne = n2v_tsner.fit_transform(n2v_embeds)\n",
    "\n",
    "plt.scatter(n2v_tsne[:, 0], n2v_tsne[:, 1], c=attrs, cmap='Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.751Z"
    }
   },
   "outputs": [],
   "source": [
    "perfs = []\n",
    "clusts = []\n",
    "for _ in range(10):\n",
    "    clusters = cluster.KMeans(7, n_init=250).fit_predict(n2v_embeds)\n",
    "\n",
    "    knns = [[] for _ in range(7)]\n",
    "    true_vals = [[] for _ in range(7)]\n",
    "    for i in range(len(clusters)):\n",
    "            knns[clusters[i]].append(i)\n",
    "            true_vals[attrs[i]].append(i)\n",
    "\n",
    "    intersects = []\n",
    "\n",
    "    for i in range(7):\n",
    "        intersects.append([len(set(knns[i]).intersection(set(true_vals[j]))) / len(knns[i]) for j in range(7)])\n",
    "\n",
    "    intersects = np.array(intersects)\n",
    "\n",
    "    vals = []\n",
    "    perms = list(itertools.permutations(list(range(7))))\n",
    "    for i in perms:\n",
    "        vals.append(sum(intersects[range(7), i]))\n",
    "\n",
    "    new = perms[vals.index(max(vals))]\n",
    "\n",
    "    remap = dict([(i, new[i]) for i in range(7)])\n",
    "\n",
    "    clusters = [remap[i] for i in clusters]\n",
    "\n",
    "    plt.scatter(n2v_tsne[:,0], n2v_tsne[:,1], c=clusters, cmap='Dark2')\n",
    "    plt.show()\n",
    "\n",
    "    sbs.heatmap(intersects[new, :])\n",
    "    plt.show()\n",
    "\n",
    "    print(max(vals)/7)\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.753Z"
    }
   },
   "outputs": [],
   "source": [
    "n2v_embeds = pickle.load(open('hepth/w2v_cora.pkl', 'rb'))[0]\n",
    "\n",
    "n2v_tsner = manifold.TSNE(n_components=2, n_iter=2500)\n",
    "n2v_tsne = n2v_tsner.fit_transform(n2v_embeds)\n",
    "\n",
    "plt.scatter(n2v_tsne[:, 0], n2v_tsne[:, 1], c=attrs, cmap='Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-02T14:40:38.754Z"
    }
   },
   "outputs": [],
   "source": [
    "perfs = []\n",
    "clusts = []\n",
    "for _ in range(10):\n",
    "    clusters = cluster.KMeans(7, n_init=250).fit_predict(n2v_embeds)\n",
    "\n",
    "    knns = [[] for _ in range(7)]\n",
    "    true_vals = [[] for _ in range(7)]\n",
    "    for i in range(len(clusters)):\n",
    "            knns[clusters[i]].append(i)\n",
    "            true_vals[attrs[i]].append(i)\n",
    "\n",
    "    intersects = []\n",
    "\n",
    "    for i in range(7):\n",
    "        intersects.append([len(set(knns[i]).intersection(set(true_vals[j]))) / len(knns[i]) for j in range(7)])\n",
    "\n",
    "    intersects = np.array(intersects)\n",
    "\n",
    "    vals = []\n",
    "    perms = list(itertools.permutations(list(range(7))))\n",
    "    for i in perms:\n",
    "        vals.append(sum(intersects[range(7), i]))\n",
    "\n",
    "    new = perms[vals.index(max(vals))]\n",
    "\n",
    "    remap = dict([(i, new[i]) for i in range(7)])\n",
    "\n",
    "    clusters = [remap[i] for i in clusters]\n",
    "\n",
    "    plt.scatter(n2v_tsne[:,0], n2v_tsne[:,1], c=clusters, cmap='Dark2')\n",
    "    plt.show()\n",
    "\n",
    "    sbs.heatmap(intersects[new, :])\n",
    "    plt.show()\n",
    "\n",
    "    print(max(vals)/7)\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
