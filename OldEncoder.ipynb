{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T14:31:31.940862Z",
     "start_time": "2019-06-12T14:31:29.818712Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from IPython.display import Image, Audio, clear_output\n",
    "\n",
    "import geopandas\n",
    "\n",
    "import pickle\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import time\n",
    "\n",
    "from notify_run import Notify\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn import cluster\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imageio\n",
    "\n",
    "import shutil\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import community as cm\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T14:31:32.043337Z",
     "start_time": "2019-06-12T14:31:31.942805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "GPU = True\n",
    "device_idx = 1\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx - 1) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T14:33:10.410351Z",
     "start_time": "2019-06-12T14:33:10.405400Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphGen(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=256):\n",
    "        super(GraphGen, self).__init__()\n",
    "    \n",
    "        self.linear1 = nn.Linear(input_dim, 128 * 128 * 256)\n",
    "        self.conv1 = nn.ConvTranspose2d(256, 128, 5)\n",
    "        self.conv2 = nn.ConvTranspose2d(128, 64, 5)\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 1, 3)\n",
    "                \n",
    "        self.activ = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = x.view(1, 256, 128, 128)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "#         x = x.view(x.shape[2], x.shape[2])\n",
    "#         x = torch.squeeze(x)\n",
    "        \n",
    "#         x = torch.round(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T14:37:10.438291Z",
     "start_time": "2019-06-12T14:37:09.624687Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [138, 138]], which is output 0 of SqueezeBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0322d8474d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [138, 138]], which is output 0 of SqueezeBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "model = GraphGen(16)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "in_vec = torch.randn(16)\n",
    "\n",
    "for i in range(50):\n",
    "    optim.zero_grad()\n",
    "    adj_mat = model(in_vec)\n",
    "    adj_mat = adj_mat.clone().squeeze()\n",
    "    loss = torch.trace(torch.pow(adj_mat, 3))\n",
    "    \n",
    "    diag = np.arange(adj_mat.shape[0])\n",
    "    \n",
    "    adj_mat[diag,diag] = 0\n",
    "    loss /= torch.sum(adj_mat)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step\n",
    "    \n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
